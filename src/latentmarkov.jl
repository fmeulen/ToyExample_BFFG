wdir = @__DIR__
cd(wdir)
outdir= joinpath(wdir, "out")

using StatsBase, Plots, LinearAlgebra
using Optim
using ForwardDiff
using Distributions
using ComponentArrays
using StatsFuns
using FiniteDiff
using TransformVariables, LogDensityProblems, LogDensityProblemsAD, DynamicHMC, TransformedLogDensities, Random
using MCMCDiagnosticTools, DynamicHMC.Diagnostics
using UnPack

struct ObservationTrajectory{S,T}
    X::Vector{S}  # vector of covariates
    Y::Vector{T}  # vector of responses
end
ObservationTrajectory(X) = ObservationTrajectory(X, fill(missing, length(X)))


# kernel K 
Ki(Œ∏,x) = [softmax([0.0, dot(x,Œ∏.Œ≥12), -Inf])' ; softmax([dot(x,Œ∏.Œ≥21), 0.0, dot(x,Œ∏.Œ≥23)])' ; softmax([-Inf, dot(x,Œ∏.Œ≥32), 0])']
# observation kernel

# kernel Œõ (observations)
œà(x) = 2.0*logistic.(cumsum(x)) .- 1.0

function response(Z) 
    Œª = œà(Z)
    [1.0-Œª[1] Œª[1]; 1.0-Œª[2] Œª[2]; 1.0-Œª[3] Œª[3]]
end
Œõi(Œ∏) =[ response(Œ∏.Z1), response(Œ∏.Z2), response(Œ∏.Z3), response(Œ∏.Z4)    ]

function generate_track(Œ∏, ùí™::ObservationTrajectory, Œ†root)             # Generate exact track + observations
    X = ùí™.X
    Œõ = Œõi(Œ∏)
    uprev = sample(Weights(Œ†root))                  # sample x0
    U = [uprev]
    Y = [ [sample(Weights(Œõ[i][uprev,:])) for i in eachindex(Œõ)] ]
    for i=eachindex(X)
        u = sample(Weights(Ki(Œ∏,X[i])[uprev,:]))         # Generate sample from previous state
        push!(U, u)
        y =  [sample(Weights(Œõ[i][u,:])) for i in eachindex(Œõ)] 
        push!(Y, y)
        uprev = u
    end
    (U, Y)
end

function h_from_observation(Œ∏, y::Vector)
    U = Œõi(Œ∏)
    u = [U[i][:,y[i]] for i in eachindex(y)]  # only those indices where y is not missing, for an index where it is missing we can just send [1;1;1;1]
    u2 = hcat(u...)
    vec(prod(u2, dims=2))
end

#h_from_observation(Œ∏, Y[1])

function normalise!(x)
    s = sum(x)
    x .= x/s
    log(s)
end

function loglik_and_bif(Œ∏, Œ†root, ùí™::ObservationTrajectory)
    @unpack X, Y = ùí™
    N = length(Y) - 1
    hprev = h_from_observation(Œ∏, Y[N+1])
    H = [hprev]
    loglik = zero(Œ∏[1][1])
    for i=N:-1:1
        h = (Ki(Œ∏,X[i]) * hprev) .* h_from_observation(Œ∏, Y[i])
        c = normalise!(h)
        loglik += c
        pushfirst!(H, ForwardDiff.value.(h))
        hprev = h
    end
    loglik += log(Œ†root' * hprev)
    (ll=loglik, H=H)          
end

function loglik(Œ∏, Œ†root, ùí™::ObservationTrajectory)
    @unpack X, Y = ùí™
    N = length(Y) - 1
    hprev = h_from_observation(Œ∏, Y[N+1])
    loglik = zero(Œ∏[1][1])
    for i=N:-1:1
        h = (Ki(Œ∏,X[i]) * hprev) .* h_from_observation(Œ∏, Y[i])
        c = normalise!(h)
        loglik += c
        hprev = h
    end
    loglik + log(Œ†root' * hprev)
end

# loglik for multiple persons
function loglik(Œ∏, Œ†root, ùí™s::Vector)
    ll = zero(Œ∏[1][1])
    for i ‚àà eachindex(ùí™s)
        ll += loglik(Œ∏, Œ†root, ùí™s[i])
    end
    ll
end


negloglik(Œ†root, ùí™) = (Œ∏) ->  -loglik_and_bif(Œ∏, Œ†root, ùí™).ll
‚àánegloglik(Œ†root, ùí™) = (Œ∏) -> ForwardDiff.gradient(negloglik(Œ†root, ùí™), Œ∏)
loglik(Œ†root, ùí™) = (Œ∏) -> loglik(Œ∏, Œ†root, ùí™) 


function guided_track(Œ∏, Œ†root, ùí™, H)# Generate approximate track
    X = ùí™.X
    N = length(H) - 1
    uprev = sample(Weights(Œ†root .* H[1])) # Weighted prior distribution
    u·µí = [uprev]
    for i=1:N
            p = Ki(Œ∏,X[i])[uprev,:] .* H[i+1]         # Weighted transition density
            u = sample(Weights(p))
            push!(u·µí, u)
            uprev = u
    end
    u·µí
end





# True parameter vector
Œ∏0 = ComponentArray(Œ≥12 = rand(2), Œ≥21 = rand(2), Œ≥23 = rand(2), Œ≥32 = rand(2), 
    Z1=rand(Exponential(1.0),3), Z2=rand(Exponential(1.0),3), Z3=rand(Exponential(1.0),3), Z4=rand(Exponential(1.0),3))
Œ†root = [1.0, 1.0, 1.0]/3.0

N = 100
X = [rand(2) for i in 1:N]
ùí™ = ObservationTrajectory(X)


# generate track  
U, Y =  generate_track(Œ∏0, ùí™, Œ†root) 
ùí™ = ObservationTrajectory(X,Y)
# backward filter
ll, H = loglik_and_bif(Œ∏0, Œ†root, ùí™)
# sample from conditioned process
U·µí = guided_track(Œ∏0, Œ†root, ùí™, H)
# separately compute loglikelihood
loglik(Œ†root, ùí™)(Œ∏0)
ùí™s = [ùí™, ùí™]
loglik(Œ†root, ùí™s)(Œ∏0)


# plotting 
N = length(U·µí)
ts = 0:(N-1)
pl_paths = plot(ts, U·µí, label="recovered")
plot!(pl_paths, ts, U, label="latent", linestyle=:dash)
#plot!(pl_paths, ts, ys .+ 1, label="observed")
pl_paths

#----------------------

der_fwdiff = ‚àánegloglik(Œ†root, ùí™)(Œ∏0)
der_findiff = FiniteDiff.finite_difference_gradient(negloglik(Œ†root, ùí™), Œ∏0)

@show der_fwdiff-der_findiff

#----------------------------------
# compute mle and plot with loglikelihood
# grid = 0:0.01:1
# Œ∏grid = [ComponentArray(p=x, q=Œ∏0.q, r=Œ∏0.r) for x in grid]
# pl_lik = plot(grid, negloglik(Œ†root, ys).(Œ∏grid), label="neg. loglikelihood")
# # out = optimize(negloglik(Œ†root, ys), 0.0, 1.0)    # box constrained optimization
# # vline!(pl_lik, [out.minimizer], label="mle")
#  vline!(pl_lik, [Œ∏0.p], label="true")

# someŒ∏ = ComponentArray(p=0.5, q=0.7, r=0.5)
# negloglik(Œ†root, ys)(someŒ∏)

# ‚àánegloglik(Œ†root, ys)(someŒ∏)
# FiniteDiff.finite_difference_gradient(negloglik(Œ†root, ys), someŒ∏)

# # ensure domain is ‚Ñù
# negloglik_repam(Œ†root, ys)= (Œ∏) -> negloglik(Œ†root, ys)(logistic.(Œ∏))

# opt = optimize(negloglik_repam(Œ†root, ys), someŒ∏)    # box constrained optimization
# m = logistic.(opt.minimizer)

# ‚àánegloglik_repam(Œ†root, ys) = (Œ∏) -> ForwardDiff.gradient(negloglik_repam(Œ†root, ys), Œ∏)
# ‚àánegloglik_repam(Œ†root, ys)(opt.minimizer)

optimize(negloglik(Œ†root, ùí™), Œ∏) 

# Try DynamicHMC

t = as((Œ≥12=as(Array, 2), Œ≥21=as(Array, 2), Œ≥23=as(Array, 2), Œ≥32=as(Array, 2),
         Z1=as(Array,as‚Ñù‚Çä, 3), Z2=as(Array, as‚Ñù‚Çä, 3), Z3=as(Array, as‚Ñù‚Çä, 3), Z4=as(Array, as‚Ñù‚Çä, 3)  )) 
p = loglik(Œ†root, ùí™)
P = TransformedLogDensity(t, p)
‚àáP = ADgradient(:ForwardDiff, P);


# one chain
outhmc = mcmc_with_warmup(Random.default_rng(2), ‚àáP, 100)
ps = outhmc.posterior_matrix

ps_t = transform.(t, eachcol(ps))

l = @layout [a  b;  c d ; e d]
plot(getindex.(getindex.(ps_t,:Œ≥12),1),label="Œ≥12"); 
hline!([Œ∏0.p],label="")
pl_p2 = histogram(getindex.(ps_t,:p),label=""); vline!([Œ∏0.p],label="")
pl_q = plot(getindex.(ps_t,:q),label="q"); hline!([Œ∏0.q],label="")
pl_q2 = histogram(getindex.(ps_t,:q),label=""); vline!([Œ∏0.q],label="")
pl_r = plot(getindex.(ps_t,:r),label="r"); hline!([Œ∏0.r],label="")
pl_r2 = histogram(getindex.(ps_t,:r),label=""); vline!([Œ∏0.r],label="")
plot(pl_p, pl_p2, pl_q, pl_q2, pl_r, pl_r2, layout=l)



ess, RÃÇ = ess_rhat(stack_posterior_matrices([outhmc]))
@show RÃÇ

# multiple chains
results = [mcmc_with_warmup(Random.default_rng(), ‚àáP, 1000) for _ in 1:5]

# To get the posterior for ``Œ±``, we need to use the columns of the `posterior_matrix` and
# then transform
posterior = transform.(t, eachcol(pool_posterior_matrices(results)));
ps_t = posterior

ps_t_Œ≥12 = getindex.(ps_t, :Œ≥12)
ps_t_Œ≥23 = getindex.(ps_t, :Œ≥23)
ps_t_Z2 = getindex.(ps_t, :Z2)
plot(getindex.(ps_t_Z2,1))



# Extract the parameter.
posterior_p = first.(posterior);

# check the mean
mean(posterior_p)

# check the effective sample size
ess, RÃÇ = ess_rhat(stack_posterior_matrices(results))

# NUTS-specific statistics of the first chain
summarize_tree_statistics(results[1].tree_statistics)



#######     TODO: ADD PRIORS, MAKE SENSIBLE TEST DATASET, ALLOW FOR MULTIPLE PERSONS


# 3 state-model with only transitions to neighbours possible, 4 questionaire questions

prior_sample = [œà(rand(Exponential(1.0),3)) for i in 1:1000]
ps1 = getindex.(prior_sample,1)
ps2 = getindex.(prior_sample,2)
ps3 = getindex.(prior_sample,3)
mean(ps1)
mean(ps2)
mean(ps3)
histogram(ps1)
histogram(ps2)
histogram(ps3)



# vector of covariates
Z =  

# dealing with missing values: if some y[i]==missing 


