wdir = @__DIR__
cd(wdir)
outdir= joinpath(wdir, "out")

using StatsBase, Plots, LinearAlgebra
using Optim
using ForwardDiff
using Distributions
using ComponentArrays
using StatsFuns
using Random
using DynamicHMC
using UnPack
using PDMats
using Turing
using StatsPlots
using BenchmarkTools
using StaticArrays
using NNlib # for softmax
import StatsBase.sample

# Y_i depends on U_i
# U_i depends on U_{i-1}, X_i
# u_1 depends on Œ†root(X1)

const NUM_HIDDENSTATES = 3
const DIM_COVARIATES = 2
const DIM_RESPONSE = 4

struct ObservationTrajectory{S,T}
    X::Vector{S}  # vector of covariates (each element of X contains the covariates at a particular time instance)
    Y::Vector{T}  # vector of responses (each element of Y contains a K-vector of responses to the K questions)
end
#ObservationTrajectory(X, dimY) = ObservationTrajectory(X, fill(fill(1,dimY), length(X)))  # constructor if only X is given

ObservationTrajectory(X, dimY) = ObservationTrajectory(X, fill(SA[1,1,1,1], length(X)))  # constructor if only X is given

# Prior on root node (x can be inital state)
Œ†root(x) = (@SVector ones(NUM_HIDDENSTATES))/3.0    



# transition kernel of the latent chain assuming 3 latent states
#Ki(Œ∏,x) = [StatsFuns.softmax([0.0, dot(x,Œ∏.Œ≥12), -Inf])' ; StatsFuns.softmax([dot(x,Œ∏.Œ≥21), 0.0, dot(x,Œ∏.Œ≥23)])' ; StatsFuns.softmax([-Inf, dot(x,Œ∏.Œ≥32), 0])']
# can also be done with StaticArrays

# to avoid type instability, both Ki methods should return an element of the same type 
Ki(Œ∏,x)= SMatrix{NUM_HIDDENSTATES,NUM_HIDDENSTATES}( NNlib.softmax([0.0 dot(x,Œ∏.Œ≥12) -Inf; dot(x,Œ∏.Œ≥21) 0.0 dot(x,Œ∏.Œ≥23) ; -Inf dot(x,Œ∏.Œ≥32) 0.0];dims=2) ) # slightly faster, though almost double allocation
Ki(_,::Missing) = SMatrix{NUM_HIDDENSTATES,NUM_HIDDENSTATES}(1.0I)
 
scaledandshifted_logistic(x) = 2.0logistic(x) -1.0 # function that maps [0,‚àû) to [0,1)

function pullback(Œ∏,x,h) # compute Ki(Œ∏,x)*h
    a1 = dot(StatsFuns.softmax(SA[0.0, dot(x,Œ∏.Œ≥12), -Inf]),h)
    a2 = dot(StatsFuns.softmax(SA[dot(x,Œ∏.Œ≥21), 0.0 ,dot(x,Œ∏.Œ≥23)]),h)
    a3 = dot(StatsFuns.softmax(SA[-Inf, dot(x,Œ∏.Œ≥32), 0.0]),h)
    SA[a1,a2,a3]
end
pullback(_, ::Missing,h) = h

"""
    response(Z) 
    
    make matrix [1.0-Œª[1] Œª[1]; 1.0-Œª[2] Œª[2]; 1.0-Œª[3] Œª[3]] (if 3 latent vars)

    # construct transition kernel Œõ to observations
    # Œª1, Œª2, Œª3 is formed by setting Œªi = logistic(cumsum(Z)[i])
"""
function response(Z) 
        Œª = scaledandshifted_logistic.(cumsum(Z))
        # Œõ = Matrix{eltype(Œª)}(undef , length(Œª), 2)  # 2 comes from assuming binary answers to questions
        # for k in eachindex(Œª)
        #     Œõ[k,:] =[  one(Œª[k])-Œª[k] Œª[k] ]
        # end
        # Œõ            
        SA[ one(Œª[1])-Œª[1] Œª[1];  one(Œª[2])-Œª[2] Œª[2];  one(Œª[3])-Œª[3] Œª[3]]
end

Œõi(Œ∏) = SA[ response(Œ∏.Z1), response(Œ∏.Z2), response(Œ∏.Z3), response(Œ∏.Z4)    ]    # assume 4 questions


#sample_observation(Œõ, u) =  [sample(Weights(Œõ[i][u,:])) for i in eachindex(Œõ)] # sample Y | U
sample_observation(Œõ, u) =  SA[sample(Weights(Œõ[1][u,:])), sample(Weights(Œõ[2][u,:])), sample(Weights(Œõ[3][u,:])), sample(Weights(Œõ[4][u,:])) ] # sample Y | U

"""
    sample(Œ∏, ùí™::ObservationTrajectory)             

    ùí™.X: vector of covariates, say of length n
    
    samples U_1,..., U_n and Y_1,..., Y_n, where 
    U_1 ~ Œ†root
    for i ‚â• 2 
        U_i | X_{i}, U_{i-1} ~ Row_{U_{i-1}} K(Œ∏,X_{i})
    (thus, last element of X are not used)

"""
function sample(Œ∏, X)# ùí™::ObservationTrajectory)             # Generate exact track + observations
    #X = ùí™.X
    Œõ = Œõi(Œ∏)
    uprev = sample(Weights(Œ†root(X[1])))                  # sample u1 (possibly depending on X[1])
    U = [uprev]
    for i in eachindex(X[2:end])
        u = sample(Weights(Ki(Œ∏,X[i])[uprev,:]))         # Generate sample from previous state
        push!(U, copy(u))
        uprev = u
    end
    Y = [sample_observation(Œõ, u) for u ‚àà U]
    #(U, ObservationTrajectory(ùí™.X, Y))
    U, Y
end


h_from_one_observation(Œõ, i::Int) = Œõ[:,i]

function h_from_observation(Œ∏, y) 
    Œõ = Œõi(Œ∏)
    # a1 = [Œõ[i][:,y[i]] for i in eachindex(y)]  # only those indices where y is not missing, for an index where it is missing we can just send [1;1;1;1], or simply define y as such in case of missingness
    # out = [prod(first.(a1))]
    # K = length(a1[1])
    # for k in 2:K
    #     push!(out,prod(getindex.(a1,k)) )
    # end
    # out
    h_from_one_observation(Œõ[1],y[1]) .* h_from_one_observation(Œõ[2],y[2]) .* h_from_one_observation(Œõ[3],y[3]) .* h_from_one_observation(Œõ[4],y[4])
end

h_from_observation(_, ::Missing) =  @SVector ones(NUM_HIDDENSTATES)


function normalise!(x)
    s = sum(x)
    log(s)
end


function loglik_and_bif(Œ∏, ùí™::ObservationTrajectory)
    @unpack X, Y = ùí™
    N = length(Y) 
    hprev = h_from_observation(Œ∏, Y[N])
    H = [hprev]
    loglik = zero(Œ∏[1][1])
    for i in N:-1:2
        h = pullback(Œ∏, X[i], h) .* h_from_observation(Œ∏, Y[i-1])
        c = normalise!(h)
        loglik += c
        pushfirst!(H, copy(ForwardDiff.value.(h)))
        hprev = h
    end
    loglik += log(dot(hprev, Œ†root(X[1])))
    (ll=loglik, H=H)          
end

function loglik(Œ∏, ùí™::ObservationTrajectory) 
    @unpack X, Y = ùí™
    N = length(Y) 
    h = h_from_observation(Œ∏, Y[N])
    loglik = zero(Œ∏[1][1])
    for i in N:-1:2
        h = pullback(Œ∏, X[i], h) .* h_from_observation(Œ∏, Y[i-1])
        c = normalise!(h)
        loglik += c
    end
    loglik + log(dot(h, Œ†root(X[1])))
end

# to do: make Œ†root depend on X[1]

# loglik for multiple persons
function loglik(Œ∏, ùí™s::Vector)
    ll = zero(Œ∏[1][1])
    for i ‚àà eachindex(ùí™s)
        ll += loglik(Œ∏, ùí™s[i])
    end
    ll 
end

loglik(ùí™) = (Œ∏) -> loglik(Œ∏, ùí™) 

‚àáloglik(ùí™) = (Œ∏) -> ForwardDiff.gradient(loglik(ùí™), Œ∏)

# check
function sample_guided(Œ∏, ùí™, H)# Generate approximate track
    X = ùí™.X
    N = length(H) # check -1?
    uprev = sample(Weights(Œ†root(X[1]) .* H[1])) # Weighted prior distribution
    u·µí = [uprev]
    for i=2:N
        w = Ki(Œ∏,X[i])[uprev,:] .* H[i]         # Weighted transition density
        u = sample(Weights(w))
        push!(u·µí, u)
        uprev = u
    end
    u·µí
end


########### An example, where data are generated from the model ####################

# True parameter vector
Œ≥up = 2.0; Œ≥down = -0.5
Œ≥12 = Œ≥23 = [Œ≥up, 0.0]
Œ≥21 = Œ≥32 = [Œ≥down, -0.1]
Z0 = [0.5, 1.0, 1.5]
Œ∏0 = ComponentArray(Œ≥12 = Œ≥12, Œ≥21 = Œ≥21, Œ≥23 = Œ≥23, Œ≥32 = Œ≥32, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0)

println("true vals", "  ", Œ≥up,"  ", Œ≥down,"  ", Z0)


# generate covariates
n = 20 # nr of subjects
T = 15 # nr of times at which we observe

# el1 = intensity, el2 = gender



TX = Union{Missing, SVector{DIM_COVARIATES,Float64}} # indien er missing vals zijn 
TY = Union{Missing, SVector{DIM_RESPONSE, Int64}}

# TX = SVector{2,Float64}
# TY = SVector{DIM_RESPONSE, Int64}

ùí™s = ObservationTrajectory{TX, TY}[]
for i in 1:n
    #local X 
    X = TX[]   # next, we can push! elements to X
    if i ‚â§ 10 
        for t in 1: T
            push!(X, SA[-0.05*t + 0.2*randn(), 1.0])
        end
    else
        for t in 1: T
            push!(X, SA[-0.05*t + 0.2*randn(), 1.0])
        end
        X[3] = missing
    end
    U, Y =  sample(Œ∏0, X) 
    YY = TY[]
    push!(YY, missing) 
    for t in  2:(T-1)
        push!(YY, Y[t]) 
    end    
    
    push!(ùí™s, ObservationTrajectory(X, YY))
end




# use of Turing to sample from the posterior


@model function logtarget(ùí™s)
    Œ≥12 ~ filldist(Normal(0,2), DIM_COVARIATES)#MvNormal(fill(0.0, 2), 2.0 * I)
    Œ≥21  ~ filldist(Normal(0,2), DIM_COVARIATES)  #MvNormal(fill(0.0, 2), 2.0 * I)
    Z0 ~ filldist(Exponential(), NUM_HIDDENSTATES) 
    Turing.@addlogprob! loglik(ComponentArray(Œ≥12 = Œ≥12, Œ≥21 = Œ≥21, Œ≥23 = Œ≥12, Œ≥32 = Œ≥21, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0), ùí™s)
end


model = logtarget(ùí™s)

# compute map and mle 
@time map_estimate = optimize(model, MAP())
@time mle_estimate = optimize(model, MLE())

println("true vals", "  ", Œ≥12,"  ", Œ≥21,"  ", Z0)

#sampler = DynamicNUTS() # HMC(0.05, 10);
sampler = NUTS()


#@time chain = sample(model, sampler, 1_00, init_params = map_estimate.values.array; progress=false);
@time chain = sample(model, sampler, 1000)#; progress=true);

# plotting 
histogram(chain)
savefig("latentmarkov_histograms.png")
plot(chain)
savefig("latentmarkov_traceplots_histograms.png")

@show chain 
# describe(chain)[1]
# describe(chain)[2]





