wdir = @__DIR__
cd(wdir)
outdir= joinpath(wdir, "out")

using StatsBase, Plots, LinearAlgebra
using Optim
using ForwardDiff
using Distributions
using ComponentArrays
using StatsFuns
using Random
using DynamicHMC
using UnPack
using PDMats
using Turing
using StatsPlots

import StatsBase.sample
struct ObservationTrajectory{S,T}
    X::Vector{S}  # vector of covariates (each element of X contains the covariates at a particular time instance)
    Y::Vector{T}  # vector of responses (each element of Y contains a K-vector of responses to the K questions)
end
ObservationTrajectory(X, dimY) = ObservationTrajectory(X, fill(fill(1,dimY), length(X)))  # constructor if only X is given


# transition kernel of the latent chain
Ki(Î¸,x) = [softmax([0.0, dot(x,Î¸.Î³12), -Inf])' ; softmax([dot(x,Î¸.Î³21), 0.0, dot(x,Î¸.Î³23)])' ; softmax([-Inf, dot(x,Î¸.Î³32), 0])']

# construct transition kernel Î› to observations
# we assume each element of the vector Z is nonnegative. A prior on 
# Î»1, Î»2, Î»3 is formed by setting Î»i = logistic(cumsum(Z)[i])
# TODO consider better motivated choices

scaledandshifted_logistic(x) = 2.0logistic(x) -1.0 # function that maps [0,âˆ) to [0,1)
"""
    make matrix [1.0-Î»[1] Î»[1]; 1.0-Î»[2] Î»[2]; 1.0-Î»[3] Î»[3]] (if 3 latent vars)
"""
function response(Z) 
        Î» = scaledandshifted_logistic.(cumsum(Z))
        Î› = Matrix{eltype(Î»)}(undef , length(Î»), 2)  # 2 comes from assuming binary answers to questions
        for k in eachindex(Î»)
            Î›[k,:] =[  one(Î»[k])-Î»[k] Î»[k] ]
        end
        Î›            
end

Î›i(Î¸) =[ response(Î¸.Z1), response(Î¸.Z2), response(Î¸.Z3), response(Î¸.Z4)    ]


sample_observation(Î›, u) =  [sample(Weights(Î›[i][u,:])) for i in eachindex(Î›)] 

"""
    sample(Î¸, ğ’ª::ObservationTrajectory, Î root)             

    ğ’ª.X: vector of covariates, say of length n
    
    samples U_1,..., U_n and Y_1,..., Y_n, where 
    U_1 ~ Î root
    for i â‰¥ 2 
        U_i | X_{i-1}, U_{i-1} ~ Row_{U_{i-1}} K(Î¸,X_{i-1})
    (thus, last element of X are not used)

"""
function sample(Î¸, ğ’ª::ObservationTrajectory, Î root)             # Generate exact track + observations
    X = ğ’ª.X
    Î› = Î›i(Î¸)
    uprev = sample(Weights(Î root))                  # sample x0
    U = [uprev]
    for i in eachindex(X[1:(end-1)])
        u = sample(Weights(Ki(Î¸,X[i])[uprev,:]))         # Generate sample from previous state
        push!(U, copy(u))
        uprev = u
    end
    Y = [sample_observation(Î›, u) for u âˆˆ U]
    (U, ObservationTrajectory(ğ’ª.X, Y))
end


function h_from_observation(Î¸::TÎ¸, y::Vector{T}) where {TÎ¸,T} 
    Î› = Î›i(Î¸)
    a1 = [Î›[i][:,y[i]] for i in eachindex(y)]  # only those indices where y is not missing, for an index where it is missing we can just send [1;1;1;1], or simply define y as such in case of missingness
    out = [prod(first.(a1))]
    K = length(a1[1])
    for k in 2:K
        push!(out,prod(getindex.(a1,k)) )
    end
    out
end

function normalise!(x)
    s = sum(x)
    x .= x/s
    log(s)
end

function loglik_and_bif(Î¸, Î root, ğ’ª::ObservationTrajectory)
    @unpack X, Y = ğ’ª
    N = length(Y) 
    hprev = h_from_observation(Î¸, Y[N])
    H = [hprev]
    loglik = zero(Î¸[1][1])
    for i in (N-1):-1:1
        h = (Ki(Î¸,X[i]) * hprev) .* h_from_observation(Î¸, Y[i])
        c = normalise!(h)
        loglik += c
        pushfirst!(H, copy(ForwardDiff.value.(h)))
        hprev = h
    end
    loglik += log(dot(hprev, Î root))
    (ll=loglik, H=H)          
end



# function loglik(Î¸::TÎ¸, Î root::TÎ , ğ’ª::ObservationTrajectory) where {TÎ¸, TÎ }
#     @unpack X, Y = ğ’ª
#     N = length(Y) - 1
#     hprev = h_from_observation(Î¸, Y[N+1])
#     loglik = zero(Î¸[1][1])
#     for i=N:-1:1
#         h = (Ki(Î¸,X[i]) * hprev) .* h_from_observation(Î¸, Y[i])
#         c = normalise!(h)
#         loglik += c
#         hprev = h
#     end
#     loglik + log(Î root' * hprev)
# end

function loglik(Î¸::TÎ¸, Î root::TÎ , ğ’ª::ObservationTrajectory) where {TÎ¸, TÎ }
    @unpack X, Y = ğ’ª
    N = length(Y) 
    h = h_from_observation(Î¸, Y[N])
    loglik = zero(Î¸[1][1])
    for i in (N-1):-1:1
        h = (Ki(Î¸,X[i]) * h) .* h_from_observation(Î¸, Y[i])
        c = normalise!(h)
        loglik += c
    end
    loglik + log(dot(h, Î root))
end





# loglik for multiple persons
function loglik(Î¸, Î root, ğ’ªs::Vector)
    ll = zero(Î¸[1][1])
    for i âˆˆ eachindex(ğ’ªs)
        ll += loglik(Î¸, Î root, ğ’ªs[i])
    end
    ll 
end

loglik(Î root, ğ’ª) = (Î¸) -> loglik(Î¸, Î root, ğ’ª) 

âˆ‡loglik(Î root, ğ’ª) = (Î¸) -> ForwardDiff.gradient(loglik(Î root, ğ’ª), Î¸)


function sample_guided(Î¸, Î root, ğ’ª, H)# Generate approximate track
    X = ğ’ª.X
    N = length(H) - 1
    uprev = sample(Weights(Î root .* H[1])) # Weighted prior distribution
    uáµ’ = [uprev]
    for i=1:N
            w = Ki(Î¸,X[i])[uprev,:] .* H[i+1]         # Weighted transition density
            u = sample(Weights(w))
            push!(uáµ’, u)
            uprev = u
    end
    uáµ’
end


########### An example, where data are generated from the model ####################

# True parameter vector
Î³up = 2.0; Î³down = -0.5
Î³12 = Î³23 = [Î³up, 0.0]
Î³21 = Î³32 = [Î³down, -0.1]
Z0 = [0.5, 1.0, 1.5]
Î¸0 = ComponentArray(Î³12 = Î³12, Î³21 = Î³21, Î³23 = Î³23, Î³32 = Î³32, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0)

println("true vals", "  ", Î³up,"  ", Î³down,"  ", Z0)

# Prior on root node
#Î root = [1.0, 1.0, 1.0]/3.0
Î root = [1.0, 0.0, 0.0]

# generate covariates
n = 40 # nr of subjects
T = 30 # nr of times at which we observe

# el1 = intensity, el2 = gender
X = [ [0.05*t + 0.2*randn(), 0.0] for t in 1:T]
dimY = 4
ğ’ªs = [ObservationTrajectory(X,dimY)]
for i in 2:n
    if i â‰¤ 10 
        X = [ [0.05*t + 0.2*randn(), 0.0] for t in 1:T]
    else
        X = [ [-0.05*t + 0.2*randn(), 1.0] for t in 1:T]
    end
    push!(ğ’ªs, ObservationTrajectory(X, dimY))
end

# generate tracks for all individuals
for i in eachindex(ğ’ªs)
    U, ğ’ª =  sample(Î¸0, ğ’ªs[i], Î root) 
    ğ’ªs[i] = ğ’ª
end 


######### testing the code ################
# generate track for one person
U, ğ’ª =  sample(Î¸0, ğ’ªs[1], Î root) 

# backward filter
ll, H = loglik_and_bif(Î¸0, Î root, ğ’ª)
# sample from conditioned process
Uáµ’ = sample_guided(Î¸0, Î root, ğ’ª, H)
# compute loglikelihood
loglik(Î root, ğ’ªs)(Î¸0)

# plotting 
N = length(Uáµ’)
ts = 1:N
Uáµ’ = sample_guided(Î¸0, Î root, ğ’ª, H)
pl_paths = plot(ts, Uáµ’, label="recovered")
plot!(pl_paths, ts, U, label="latent", linestyle=:dash)

pl_paths


######### end testing the code ################






#---------------------- check computing times
@time loglik(Î root, ğ’ªs)(Î¸0);
@time âˆ‡loglik(Î root, ğ’ªs)(Î¸0);

####### ForwardDiff is faster and allocates less than FiniteDiff ###########
TESTING = false
if TESTING
    using FiniteDiff
    using BenchmarkTools
    âˆ‡loglik_fd(Î root, ğ’ª) = (Î¸) -> FiniteDiff.finite_difference_gradient(loglik(Î root, ğ’ª), Î¸)
    @btime âˆ‡loglik_fd(Î root, ğ’ª)(Î¸0);
    @btime âˆ‡loglik(Î root, ğ’ª)(Î¸0);
end
##########################################################################
# use of Turing to sample from the posterior

@model function logtarget(ğ’ªs, Î root)
    Î³up ~ Normal(0,3)
    Î³down ~ Normal(0,3)
    Î³12 = Î³23 = [Î³up, 0.0]
    Î³21 = Î³32 = [Î³down, -0.1]
    Z0 ~ filldist(Exponential(), 3) 
    Turing.@addlogprob! loglik(Î root, ğ’ªs)(ComponentArray(Î³12 = Î³12, Î³21 = Î³21, Î³23 = Î³23, Î³32 = Î³32, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0))
end

# @model function logtarget2(ğ’ªs, Î root, K)  # K is nr of latent states (turns out that be much slower)
#     Î³up ~ Normal(0,3)
#     Î³down ~ Normal(0,3)
#     Î³12 = Î³23 = [Î³up, 0.0]
#     Î³21 = Î³32 = [Î³down, -0.1]
#     r ~ Dirichlet(fill(1,K+1))
#    # r ~ filldist(Gamma(2.0,1.0), K+1)
#     Z0 = cumsum(r)[1:K] #/sum(r)
#     Turing.@addlogprob! loglik(Î root, ğ’ªs)(ComponentArray(Î³12 = Î³12, Î³21 = Î³21, Î³23 = Î³23, Î³32 = Î³32, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0))
# end





# multiple samplers to choose from, such as 
sampler = DynamicNUTS() # HMC(0.05, 10);
model = logtarget(ğ’ªs, Î root)

# compute map and mle 
@time map_estimate = optimize(model, MAP())
#@time mle_estimate = optimize(model, MLE())

println("true vals", "  ", Î³up,"  ", Î³down,"  ", Z0)

@time chain = sample(model, sampler, 1_000, init_params = map_estimate.values.array; progress=false);
#@time chain = sample(model, sampler, 1_000)#; progress=true);

# plotting 
histogram(chain)
savefig("latentmarkov_histograms.png")
plot(chain)
savefig("latentmarkov_traceplots_histograms.png")

describe(chain)[1]
describe(chain)[2]






# TODO: profiling
# using ProfileView

# ProfileView.@profview loglik(Î¸0, Î root, ğ’ªs)
# ProfileView.@profview âˆ‡loglik(Î root, ğ’ª)(Î¸0)

# @code_warntype loglik(Î¸0, Î root, ğ’ªs[1])
# @code_warntype loglik(Î¸0, Î root, ğ’ªs)

# y =ğ’ªs[1].Y[2]
# Î¸ = Î¸0
# @code_warntype h_from_observation(Î¸, y)

# @code_warntype âˆ‡loglik(Î root, ğ’ªs[1])(Î¸0);

#using BenchmarkTools


# l = @layout [a  b;  c d ; e d]
# getindex.(getindex.(ps_t,:Î³12),2)
# getindex.(getindex.(ps_t,:Z1),3)
# plot(getindex.(getindex.(ps_t,:Î³12),1),label="Î³12"); 
# hline!([Î¸0.p],label="")
# pl_p2 = histogram(getindex.(ps_t,:p),label=""); vline!([Î¸0.p],label="")
# pl_q = plot(getindex.(ps_t,:q),label="q"); hline!([Î¸0.q],label="")
# pl_q2 = histogram(getindex.(ps_t,:q),label=""); vline!([Î¸0.q],label="")
# pl_r = plot(getindex.(ps_t,:r),label="r"); hline!([Î¸0.r],label="")
# pl_r2 = histogram(getindex.(ps_t,:r),label=""); vline!([Î¸0.r],label="")
# plot(pl_p, pl_p2, pl_q, pl_q2, pl_r, pl_r2, layout=l)

