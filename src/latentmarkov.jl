wdir = @__DIR__
cd(wdir)
outdir= joinpath(wdir, "out")

using StatsBase, Plots, LinearAlgebra
using Optim
using ForwardDiff
using Distributions
using ComponentArrays
using StatsFuns
using Random
using DynamicHMC
using UnPack
using PDMats
using Turing
using StatsPlots
using BenchmarkTools
using StaticArrays
using NNlib # for softmax

import StatsBase.sample

struct ObservationTrajectory{S,T}
    X::Vector{S}  # vector of covariates (each element of X contains the covariates at a particular time instance)
    Y::Vector{T}  # vector of responses (each element of Y contains a K-vector of responses to the K questions)
end
#ObservationTrajectory(X, dimY) = ObservationTrajectory(X, fill(fill(1,dimY), length(X)))  # constructor if only X is given

ObservationTrajectory(X, dimY) = ObservationTrajectory(X, fill(SA[1,1,1,1], length(X)))  # constructor if only X is given


# transition kernel of the latent chain assuming 3 latent states
#Ki(Î¸,x) = [StatsFuns.softmax([0.0, dot(x,Î¸.Î³12), -Inf])' ; StatsFuns.softmax([dot(x,Î¸.Î³21), 0.0, dot(x,Î¸.Î³23)])' ; StatsFuns.softmax([-Inf, dot(x,Î¸.Î³32), 0])']
# can also be done with StaticArrays
Ki(Î¸,x)= NNlib.softmax([0.0 dot(x,Î¸.Î³12) -Inf; dot(x,Î¸.Î³21) 0.0 dot(x,Î¸.Î³23) ; -Inf dot(x,Î¸.Î³32) 0];dims=2)  # slightly faster, though almost double allocation
 
 
scaledandshifted_logistic(x) = 2.0logistic(x) -1.0 # function that maps [0,âˆ) to [0,1)

"""
    response(Z) 
    
    make matrix [1.0-Î»[1] Î»[1]; 1.0-Î»[2] Î»[2]; 1.0-Î»[3] Î»[3]] (if 3 latent vars)

    # construct transition kernel Î› to observations
    # Î»1, Î»2, Î»3 is formed by setting Î»i = logistic(cumsum(Z)[i])
"""
function response(Z) 
        Î» = scaledandshifted_logistic.(cumsum(Z))
        # Î› = Matrix{eltype(Î»)}(undef , length(Î»), 2)  # 2 comes from assuming binary answers to questions
        # for k in eachindex(Î»)
        #     Î›[k,:] =[  one(Î»[k])-Î»[k] Î»[k] ]
        # end
        # Î›            
        SA[ one(Î»[1])-Î»[1] Î»[1];  one(Î»[2])-Î»[2] Î»[2];  one(Î»[3])-Î»[3] Î»[3]]
end

Î›i(Î¸) = SA[ response(Î¸.Z1), response(Î¸.Z2), response(Î¸.Z3), response(Î¸.Z4)    ]    # assume 4 questions


#sample_observation(Î›, u) =  [sample(Weights(Î›[i][u,:])) for i in eachindex(Î›)] # sample Y | U
sample_observation(Î›, u) =  SA[sample(Weights(Î›[1][u,:])), sample(Weights(Î›[2][u,:])), sample(Weights(Î›[3][u,:])), sample(Weights(Î›[4][u,:])) ] # sample Y | U

"""
    sample(Î¸, ğ’ª::ObservationTrajectory, Î root)             

    ğ’ª.X: vector of covariates, say of length n
    
    samples U_1,..., U_n and Y_1,..., Y_n, where 
    U_1 ~ Î root
    for i â‰¥ 2 
        U_i | X_{i}, U_{i-1} ~ Row_{U_{i-1}} K(Î¸,X_{i})
    (thus, last element of X are not used)

"""
function sample(Î¸, ğ’ª::ObservationTrajectory, Î root)             # Generate exact track + observations
    X = ğ’ª.X
    Î› = Î›i(Î¸)
    uprev = sample(Weights(Î root))                  # sample u1 (possibly depending on X[1])
    U = [uprev]
    for i in eachindex(X[2:end])
        u = sample(Weights(Ki(Î¸,X[i])[uprev,:]))         # Generate sample from previous state
        push!(U, copy(u))
        uprev = u
    end
    Y = [sample_observation(Î›, u) for u âˆˆ U]
    (U, ObservationTrajectory(ğ’ª.X, Y))
end


function h_from_observation(Î¸::TÎ¸, y::T) where {TÎ¸,T} 
    Î› = Î›i(Î¸)
    # a1 = [Î›[i][:,y[i]] for i in eachindex(y)]  # only those indices where y is not missing, for an index where it is missing we can just send [1;1;1;1], or simply define y as such in case of missingness
    # out = [prod(first.(a1))]
    # K = length(a1[1])
    # for k in 2:K
    #     push!(out,prod(getindex.(a1,k)) )
    # end
    # out
    Î›[1][:,y[1]] .* Î›[2][:,y[2]] .* Î›[3][:,y[3]] .* Î›[4][:,y[4]]
end

function normalise!(x)
    s = sum(x)
    log(s)
end


function loglik_and_bif(Î¸, Î root, ğ’ª::ObservationTrajectory)
    @unpack X, Y = ğ’ª
    N = length(Y) 
    hprev = h_from_observation(Î¸, Y[N])
    H = [hprev]
    loglik = zero(Î¸[1][1])
    for i in N:-1:2
        h = (Ki(Î¸,X[i]) * hprev) .* h_from_observation(Î¸, Y[i-1])
        c = normalise!(h)
        loglik += c
        pushfirst!(H, copy(ForwardDiff.value.(h)))
        hprev = h
    end
    loglik += log(dot(hprev, Î root))
    (ll=loglik, H=H)          
end

function loglik(Î¸::TÎ¸, Î root::TÎ , ğ’ª::ObservationTrajectory) where {TÎ¸, TÎ }
    @unpack X, Y = ğ’ª
    N = length(Y) 
    h = h_from_observation(Î¸, Y[N])
    loglik = zero(Î¸[1][1])
    for i in N:-1:2
        K = Ki(Î¸,X[i]) 
       # K = @SMatrix ones(3,3)
        h = (K * h) .* h_from_observation(Î¸, Y[i-1])
        #@show typeof(h)
        c = normalise!(h)
        loglik += c
    end
    loglik + log(dot(h, Î root))
end

# to do: make Î root depend on X[1]

# loglik for multiple persons
function loglik(Î¸, Î root, ğ’ªs::Vector)
    ll = zero(Î¸[1][1])
    for i âˆˆ eachindex(ğ’ªs)
        ll += loglik(Î¸, Î root, ğ’ªs[i])
    end
    ll 
end

loglik(Î root, ğ’ª) = (Î¸) -> loglik(Î¸, Î root, ğ’ª) 

âˆ‡loglik(Î root, ğ’ª) = (Î¸) -> ForwardDiff.gradient(loglik(Î root, ğ’ª), Î¸)

# check
function sample_guided(Î¸, Î root, ğ’ª, H)# Generate approximate track
    X = ğ’ª.X
    N = length(H) # check -1?
    uprev = sample(Weights(Î root .* H[1])) # Weighted prior distribution
    uáµ’ = [uprev]
    for i=2:N
            w = Ki(Î¸,X[i])[uprev,:] .* H[i]         # Weighted transition density
            u = sample(Weights(w))
            push!(uáµ’, u)
            uprev = u
    end
    uáµ’
end


########### An example, where data are generated from the model ####################

# True parameter vector
Î³up = 2.0; Î³down = -0.5
Î³12 = Î³23 = [Î³up, 0.0]
Î³21 = Î³32 = [Î³down, -0.1]
Z0 = [0.5, 1.0, 1.5]
Î¸0 = ComponentArray(Î³12 = Î³12, Î³21 = Î³21, Î³23 = Î³23, Î³32 = Î³32, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0)

println("true vals", "  ", Î³up,"  ", Î³down,"  ", Z0)

# Prior on root node
#Î root = SA[1.0, 1.0, 1.0]/3.0
Î root = SA[1.0, 0.0, 0.0]

# generate covariates
n = 40 # nr of subjects
T = 30 # nr of times at which we observe

# el1 = intensity, el2 = gender
X = [ SA[0.05*t + 0.2*randn(), 0.0] for t in 1:T]
dimY = 4
ğ’ªs = [ObservationTrajectory(X,dimY)]
for i in 2:n
    local X 
    if i â‰¤ 10 
        X = [ SA[0.05*t + 0.2*randn(), 0.0] for t in 1:T]
    else
        X = [ SA[-0.05*t + 0.2*randn(), 1.0] for t in 1:T]
    end
    push!(ğ’ªs, ObservationTrajectory(X, dimY))
end

# generate tracks for all individuals
for i in eachindex(ğ’ªs)
    U, ğ’ª =  sample(Î¸0, ğ’ªs[i], Î root) 
    ğ’ªs[i] = ğ’ª
end 

# use of Turing to sample from the posterior


@model function logtarget(ğ’ªs, Î root)
    Î³12 ~ filldist(Normal(0,2),2)#MvNormal(fill(0.0, 2), 2.0 * I)
    Î³21  ~ filldist(Normal(0,2),2)  #MvNormal(fill(0.0, 2), 2.0 * I)
    Z0 ~ filldist(Exponential(), 3) 
    #Turing.@addlogprob! loglik(Î root, ğ’ªs)(Î¸)
    Turing.@addlogprob! loglik(ComponentArray(Î³12 = Î³12, Î³21 = Î³21, Î³23 = Î³12, Î³32 = Î³21, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0), Î root, ğ’ªs)
end


model = logtarget(ğ’ªs, Î root)

# compute map and mle 
@time map_estimate = optimize(model, MAP())
@time mle_estimate = optimize(model, MLE())

println("true vals", "  ", Î³12,"  ", Î³21,"  ", Z0)

#sampler = DynamicNUTS() # HMC(0.05, 10);
sampler = NUTS()


#@time chain = sample(model, sampler, 1_00, init_params = map_estimate.values.array; progress=false);
@time chain = sample(model, sampler, 1000)#; progress=true);

# plotting 
histogram(chain)
savefig("latentmarkov_histograms.png")
plot(chain)
savefig("latentmarkov_traceplots_histograms.png")

@show chain 
# describe(chain)[1]
# describe(chain)[2]





