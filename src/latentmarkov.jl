wdir = @__DIR__
cd(wdir)
outdir= joinpath(wdir, "out")

using StatsBase, Plots, LinearAlgebra
using Optim
using ForwardDiff
using Distributions
using ComponentArrays
using StatsFuns
using FiniteDiff
using TransformVariables, LogDensityProblems, LogDensityProblemsAD, DynamicHMC, TransformedLogDensities, Random
using MCMCDiagnosticTools, DynamicHMC.Diagnostics
using UnPack
using PDMats

struct ObservationTrajectory{S,T}
    X::Vector{S}  # vector of covariates
    Y::Vector{T}  # vector of responses
end
ObservationTrajectory(X) = ObservationTrajectory(X, fill(missing, length(X)))  # constructor


# kernel K 
Ki(Œ∏,x) = [softmax([0.0, dot(x,Œ∏.Œ≥12), -Inf])' ; softmax([dot(x,Œ∏.Œ≥21), 0.0, dot(x,Œ∏.Œ≥23)])' ; softmax([-Inf, dot(x,Œ∏.Œ≥32), 0])']

# kernel Œõ (observations)
œà(x) = 2.0*logistic.(cumsum(x)) .- 1.0

function response(Z) 
    Œª = œà(Z)
    [1.0-Œª[1] Œª[1]; 1.0-Œª[2] Œª[2]; 1.0-Œª[3] Œª[3]]
end
Œõi(Œ∏) =[ response(Œ∏.Z1), response(Œ∏.Z2), response(Œ∏.Z3), response(Œ∏.Z4)    ]

function generate_track(Œ∏, ùí™::ObservationTrajectory, Œ†root)             # Generate exact track + observations
    X = ùí™.X
    Œõ = Œõi(Œ∏)
    uprev = sample(Weights(Œ†root))                  # sample x0
    U = [uprev]
    Y = [ [sample(Weights(Œõ[i][uprev,:])) for i in eachindex(Œõ)] ]
    for i=eachindex(X)
        u = sample(Weights(Ki(Œ∏,X[i])[uprev,:]))         # Generate sample from previous state
        push!(U, u)
        y =  [sample(Weights(Œõ[i][u,:])) for i in eachindex(Œõ)] 
        push!(Y, y)
        uprev = u
    end
    (U, Y)
end

function h_from_observation(Œ∏, y::Vector)
    Œõ = Œõi(Œ∏)
    a1 = [Œõ[i][:,y[i]] for i in eachindex(y)]  # only those indices where y is not missing, for an index where it is missing we can just send [1;1;1;1]
    a2 = hcat(a1...)
    vec(prod(a2, dims=2))
end


function normalise!(x)
    s = sum(x)
    x .= x/s
    log(s)
end

function loglik_and_bif(Œ∏, Œ†root, ùí™::ObservationTrajectory)
    @unpack X, Y = ùí™
    N = length(Y) - 1
    hprev = h_from_observation(Œ∏, Y[N+1])
    H = [hprev]
    loglik = zero(Œ∏[1][1])
    for i=N:-1:1
        h = (Ki(Œ∏,X[i]) * hprev) .* h_from_observation(Œ∏, Y[i])
        c = normalise!(h)
        loglik += c
        pushfirst!(H, ForwardDiff.value.(h))
        hprev = h
    end
    loglik += log(Œ†root' * hprev)
    (ll=loglik, H=H)          
end

function loglik(Œ∏, Œ†root, ùí™::ObservationTrajectory)
    @unpack X, Y = ùí™
    N = length(Y) - 1
    hprev = h_from_observation(Œ∏, Y[N+1])
    loglik = zero(Œ∏[1][1])
    for i=N:-1:1
        h = (Ki(Œ∏,X[i]) * hprev) .* h_from_observation(Œ∏, Y[i])
        c = normalise!(h)
        loglik += c
        hprev = h
    end
    loglik + log(Œ†root' * hprev)
end

# loglik for multiple persons
function loglik(Œ∏, Œ†root, ùí™s::Vector)
    ll = zero(Œ∏[1][1])
    for i ‚àà eachindex(ùí™s)
        ll += loglik(Œ∏, Œ†root, ùí™s[i])
    end
    ll + logprior(Œ∏)
end


negloglik(Œ†root, ùí™) = (Œ∏) ->  -loglik_and_bif(Œ∏, Œ†root, ùí™).ll
‚àánegloglik(Œ†root, ùí™) = (Œ∏) -> ForwardDiff.gradient(negloglik(Œ†root, ùí™), Œ∏)
loglik(Œ†root, ùí™) = (Œ∏) -> loglik(Œ∏, Œ†root, ùí™) 

logpdfexp3(Z) = logpdf(Exponential(1.0), Z[1]) + logpdf(Exponential(1.0), Z[2]) + logpdf(Exponential(1.0), Z[3])

function logprior(Œ∏,Œ±=2.0)
    ùíü = MvNormal(2,Œ±)
    logprioru = logpdf(ùíü, Œ∏.Œ≥12) + logpdf(ùíü, Œ∏.Œ≥21) + logpdf(ùíü, Œ∏.Œ≥23) + logpdf(ùíü, Œ∏.Œ≥32)   
    logpriorŒª = logpdfexp3(Œ∏.Z1) + logpdfexp3(Œ∏.Z2) + logpdfexp3(Œ∏.Z3) + logpdfexp3(Œ∏.Z4)  
    logprioru + logpriorŒª 
end

‚àáloglik(Œ†root, ùí™) = (Œ∏) -> ForwardDiff.gradient(loglik(Œ†root, ùí™), Œ∏)

function guided_track(Œ∏, Œ†root, ùí™, H)# Generate approximate track
    X = ùí™.X
    N = length(H) - 1
    uprev = sample(Weights(Œ†root .* H[1])) # Weighted prior distribution
    u·µí = [uprev]
    for i=1:N
            w = Ki(Œ∏,X[i])[uprev,:] .* H[i+1]         # Weighted transition density
            u = sample(Weights(w))
            push!(u·µí, u)
            uprev = u
    end
    u·µí
end





# True parameter vector
Œ≥up = 0.7; Œ≥down = -0.8
Œ≥12 = Œ≥23 = [Œ≥up, 0.0]
Œ≥21 = Œ≥32 = [Œ≥down, -0.1]
Z0 = [0.8, 1.0, 1.5]
Œ∏0 = ComponentArray(Œ≥12 = Œ≥12, Œ≥21 = Œ≥21, Œ≥23 = Œ≥23, Œ≥32 = Œ≥32, 
    Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0)


# Prior on root node
Œ†root = [1.0, 1.0, 1.0]/3.0

# generate covariates
n = 11 # nr of subjects
T = 250 # nr of times at which we observe


# generate covariates, el1 = intensity, el2 = gender
ùí™s = []
for i in 1:11
    if i ‚â§ 5 
        X = [ [0.05*t + 0.2*randn(), 0.0] for t in 1:T]
    else
        X = [ [-0.05*t + 0.2*randn(), 1.0] for t in 1:T]
    end
    push!(ùí™s, ObservationTrajectory(X))
end

######### testing the code ################

# generate track  
U, Y =  generate_track(Œ∏0, ùí™s[1], Œ†root) 
ùí™ = ObservationTrajectory(ùí™s[1].X,Y)
# backward filter
ll, H = loglik_and_bif(Œ∏0, Œ†root, ùí™)
# sample from conditioned process
U·µí = guided_track(Œ∏0, Œ†root, ùí™, H)
# separately compute loglikelihood
loglik(Œ†root, ùí™)(Œ∏0)


# generate tracks for all individuals
for i in eachindex(ùí™s)
    U, Y =  generate_track(Œ∏0, ùí™s[i], Œ†root) 
    ùí™s[i] = ObservationTrajectory(ùí™s[i].X, Y)
end 

loglik(Œ†root, ùí™s)(Œ∏0)


# plotting 
N = length(U·µí)
ts = 0:(N-1)
pl_paths = plot(ts, U·µí, label="recovered")
plot!(pl_paths, ts, U, label="latent", linestyle=:dash)
#plot!(pl_paths, ts, ys .+ 1, label="observed")
pl_paths

#----------------------

der_fwdiff = ‚àánegloglik(Œ†root, ùí™)(Œ∏0)
der_findiff = FiniteDiff.finite_difference_gradient(negloglik(Œ†root, ùí™), Œ∏0)

@show der_fwdiff-der_findiff

@time loglik(Œ†root, ùí™s)(Œ∏0);
@time ‚àáloglik(Œ†root, ùí™s)(Œ∏0);

optimize(loglik(Œ†root, ùí™s), Œ∏0, GradientDescent())
#----------------------------------
# compute mle and plot with loglikelihood
# grid = 0:0.01:1
# Œ∏grid = [ComponentArray(p=x, q=Œ∏0.q, r=Œ∏0.r) for x in grid]
# pl_lik = plot(grid, negloglik(Œ†root, ys).(Œ∏grid), label="neg. loglikelihood")
# # out = optimize(negloglik(Œ†root, ys), 0.0, 1.0)    # box constrained optimization
# # vline!(pl_lik, [out.minimizer], label="mle")
#  vline!(pl_lik, [Œ∏0.p], label="true")

# someŒ∏ = ComponentArray(p=0.5, q=0.7, r=0.5)
# negloglik(Œ†root, ys)(someŒ∏)

# ‚àánegloglik(Œ†root, ys)(someŒ∏)
# FiniteDiff.finite_difference_gradient(negloglik(Œ†root, ys), someŒ∏)

# # ensure domain is ‚Ñù
# negloglik_repam(Œ†root, ys)= (Œ∏) -> negloglik(Œ†root, ys)(logistic.(Œ∏))

# opt = optimize(negloglik_repam(Œ†root, ys), someŒ∏)    # box constrained optimization
# m = logistic.(opt.minimizer)

# ‚àánegloglik_repam(Œ†root, ys) = (Œ∏) -> ForwardDiff.gradient(negloglik_repam(Œ†root, ys), Œ∏)
# ‚àánegloglik_repam(Œ†root, ys)(opt.minimizer)

optimize(negloglik(Œ†root, ùí™s[1]), Œ∏0) 

# Try DynamicHMC

t = as((Œ≥12=as(Array, 2), Œ≥21=as(Array, 2), Œ≥23=as(Array, 2), Œ≥32=as(Array, 2),
         Z1=as(Array,as‚Ñù‚Çä, 3), Z2=as(Array, as‚Ñù‚Çä, 3), Z3=as(Array, as‚Ñù‚Çä, 3), Z4=as(Array, as‚Ñù‚Çä, 3)  )) 
p = loglik(Œ†root, ùí™s)
P = TransformedLogDensity(t, p)
‚àáP = ADgradient(:ForwardDiff, P);


# one chain
outhmc = mcmc_with_warmup(Random.default_rng(2), ‚àáP, 100)
ps = outhmc.posterior_matrix

ps_t = transform.(t, eachcol(ps))

l = @layout [a  b;  c d ; e d]
getindex.(getindex.(ps_t,:Œ≥12),2)
getindex.(getindex.(ps_t,:Z1),3)
plot(getindex.(getindex.(ps_t,:Œ≥12),1),label="Œ≥12"); 
hline!([Œ∏0.p],label="")
pl_p2 = histogram(getindex.(ps_t,:p),label=""); vline!([Œ∏0.p],label="")
pl_q = plot(getindex.(ps_t,:q),label="q"); hline!([Œ∏0.q],label="")
pl_q2 = histogram(getindex.(ps_t,:q),label=""); vline!([Œ∏0.q],label="")
pl_r = plot(getindex.(ps_t,:r),label="r"); hline!([Œ∏0.r],label="")
pl_r2 = histogram(getindex.(ps_t,:r),label=""); vline!([Œ∏0.r],label="")
plot(pl_p, pl_p2, pl_q, pl_q2, pl_r, pl_r2, layout=l)



ess, RÃÇ = ess_rhat(stack_posterior_matrices([outhmc]))
@show RÃÇ

# multiple chains
results = [mcmc_with_warmup(Random.default_rng(), ‚àáP, 1000) for _ in 1:5]

# To get the posterior for ``Œ±``, we need to use the columns of the `posterior_matrix` and
# then transform
posterior = transform.(t, eachcol(pool_posterior_matrices(results)));
ps_t = posterior

ps_t_Œ≥12 = getindex.(ps_t, :Œ≥12)
ps_t_Œ≥23 = getindex.(ps_t, :Œ≥23)
ps_t_Z2 = getindex.(ps_t, :Z2)
plot(getindex.(ps_t_Z2,1))



# Extract the parameter.
posterior_p = first.(posterior);

# check the mean
mean(posterior_p)

# check the effective sample size
ess, RÃÇ = ess_rhat(stack_posterior_matrices(results))

# NUTS-specific statistics of the first chain
summarize_tree_statistics(results[1].tree_statistics)



#######     TODO: ADD PRIORS, MAKE SENSIBLE TEST DATASET, ALLOW FOR MULTIPLE PERSONS


# 3 state-model with only transitions to neighbours possible, 4 questionaire questions

prior_sample = [œà(rand(Exponential(1.0),3)) for i in 1:1000]
ps1 = getindex.(prior_sample,1)
ps2 = getindex.(prior_sample,2)
ps3 = getindex.(prior_sample,3)
mean(ps1)
mean(ps2)
mean(ps3)
histogram(ps1)
histogram(ps2)
histogram(ps3)



# vector of covariates
Z =  

# dealing with missing values: if some y[i]==missing 


