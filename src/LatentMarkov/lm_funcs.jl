# Y_i depends on U_i
# U_i depends on U_{i-1}, X_i
# u_1 depends on Œ†root(X1)



struct ObservationTrajectory{S,T}
    X::Vector{S}  # vector of covariates (each element of X contains the covariates at a particular time instance)
    Y::Vector{T}  # vector of responses (each element of Y contains a K-vector of responses to the K questions)
end
#ObservationTrajectory(X,  DIM_RESPONSE) = ObservationTrajectory(X, fill(fill(1,DIM_RESPONSE), length(X)))  # constructor if only X is given

ObservationTrajectory(X, _) = ObservationTrajectory(X, fill(SA[1,1,1,1], length(X)))  # constructor if only X is given

# Prior on root node (x can be inital state)
Œ†root(_) = (@SVector ones(NUM_HIDDENSTATES))/3.0    

# transition kernel of the latent chain assuming 3 latent states
#Ki(Œ∏,x) = [StatsFuns.softmax([0.0, dot(x,Œ∏.Œ≥12), -Inf])' ; StatsFuns.softmax([dot(x,Œ∏.Œ≥21), 0.0, dot(x,Œ∏.Œ≥23)])' ; StatsFuns.softmax([-Inf, dot(x,Œ∏.Œ≥32), 0])']
# to avoid type instability, both Ki methods should return an element of the same type 
# slightly faster, though almost double allocation

Ki(Œ∏,x)= SMatrix{NUM_HIDDENSTATES,NUM_HIDDENSTATES}( 
    NNlib.softmax([0.0 dot(x,Œ∏.Œ≥12) -Inf64; dot(x,Œ∏.Œ≥21) 0.0 dot(x,Œ∏.Œ≥23) ; -Inf64 dot(x,Œ∏.Œ≥32) 0.0];dims=2) ) 
    
Ki(_,::Missing) = SMatrix{NUM_HIDDENSTATES,NUM_HIDDENSTATES}(1.0I)
 
scaledandshifted_logistic(x) = 2.0logistic(x) -1.0 # function that maps [0,‚àû) to [0,1)

"""
    pullback(Œ∏,x,h)  
        
    returns  Ki(Œ∏,x)*h
"""
function pullback(Œ∏,x,h) 
    a1 = dot(StatsFuns.softmax(SA[0.0, dot(x,Œ∏.Œ≥12), -Inf64]),h)
    a2 = dot(StatsFuns.softmax(SA[dot(x,Œ∏.Œ≥21), 0.0 ,dot(x,Œ∏.Œ≥23)]),h)
    a3 = dot(StatsFuns.softmax(SA[-Inf64, dot(x,Œ∏.Œ≥32), 0.0]),h)
    SA[a1,a2,a3]
end

"""
    pullback(_, ::Missing, h)

    returns pullback in case covariates are missing
    as we assume no state change in this case, this simply returns h
"""
pullback(_, ::Missing, h) = h


mapZtoŒª(x) = scaledandshifted_logistic.(cumsum(x))

"""
    response(Z) 
    
    make matrix [1.0-Œª[1] Œª[1]; 1.0-Œª[2] Œª[2]; 1.0-Œª[3] Œª[3]] (if 3 latent vars)

    # construct transition kernel Œõ to observations
    # Œª1, Œª2, Œª3 is formed by setting Œªi = logistic(cumsum(Z)[i])
"""
function response(Z) 
        Œª = mapZtoŒª(Z)
        SA[ one(Œª[1])-Œª[1] Œª[1];  one(Œª[2])-Œª[2] Œª[2];  one(Œª[3])-Œª[3] Œª[3]]
end

# function response(Z::Vector{T}) where T  # slightly slower, but generic
#     Œª = mapZtoŒª(Z)
#     v1 = SVector{NUM_HIDDENSTATES,T}(Œª)
#     v2 = SVector{NUM_HIDDENSTATES,T}(one(T) .- Œª)
#     hcat(v2, v1)
# end


Œõi(Œ∏) = SA[ response(Œ∏.Z1), response(Œ∏.Z2), response(Œ∏.Z3), response(Œ∏.Z4)    ]    # assume 4 questions


#sample_observation(Œõ, u) =  [sample(Weights(Œõ[i][u,:])) for i in eachindex(Œõ)] # sample Y | U
sample_observation(Œõ, u) =  SA[sample(Weights(Œõ[1][u,:])), sample(Weights(Œõ[2][u,:])), sample(Weights(Œõ[3][u,:])), sample(Weights(Œõ[4][u,:])) ] # sample Y | U

"""
    sample(Œ∏, X)             

    X: vector of covariates, say of length n
    
    samples U_1,..., U_n and Y_1,..., Y_n, where 
    U_1 ~ Œ†root
    for i ‚â• 2 
        U_i | X_{i}, U_{i-1} ~ Row_{U_{i-1}} K(Œ∏,X_{i})
    (thus, last element of X are not used)

"""
function sample(Œ∏, X)            # Generate exact track + observations
    Œõ = Œõi(Œ∏)
    uprev = sample(Weights(Œ†root(X[1])))                  # sample u1 (possibly depending on X[1])
    U = [uprev]
    for i in eachindex(X[2:end])
        u = sample(Weights(Ki(Œ∏,X[i])[uprev,:]))         # Generate sample from previous state
        push!(U, copy(u))
        uprev = u
    end
    Y = [sample_observation(Œõ, u) for u ‚àà U]
    U, Y
end


h_from_one_observation(Œõ, i::Int) = Œõ[:,i]

"""
    h_from_observation(Œ∏, y) 

    returns message sent by observation y, when the parameter vector is Œ∏
"""
function h_from_observation(Œ∏, y) 
    Œõ = Œõi(Œ∏)
    # a1 = [Œõ[i][:,y[i]] for i in eachindex(y)]  # only those indices where y is not missing, for an index where it is missing we can just send [1;1;1;1], or simply define y as such in case of missingness
    # out = [prod(first.(a1))]
    # K = length(a1[1])
    # for k in 2:K
    #     push!(out,prod(getindex.(a1,k)) )
    # end
    # out
    h_from_one_observation(Œõ[1],y[1]) .* h_from_one_observation(Œõ[2],y[2]) .* h_from_one_observation(Œõ[3],y[3]) .* h_from_one_observation(Œõ[4],y[4])
end

h_from_observation(_, ::Missing) =  @SVector ones(NUM_HIDDENSTATES)


"""
    normalise!(x)

    inplace change of x to x/sum(x)
    returns log(sum(x))
"""
function normalise(x)
    s = sum(x)
    x/s, log(s)
end


"""
    loglik_and_bif(Œ∏, ùí™::ObservationTrajectory)

    Compute loglikelihood and h-vectors from the backward information filter, for one ObservationTrajectory
"""
function loglik_and_bif(Œ∏, ùí™::ObservationTrajectory)
    @unpack X, Y = ùí™
    N = length(Y) 
    h = h_from_observation(Œ∏, Y[N])
    H = [h]
    loglik = zero(Œ∏[1][1])
    for i in N:-1:2
        h = pullback(Œ∏, X[i], h) .* h_from_observation(Œ∏, Y[i-1])
#        c = normalise!(h)
        h, c = normalise(h)
        loglik += c
        pushfirst!(H, copy(ForwardDiff.value.(h)))
        #hprev = h
    end
    loglik += log(dot(h, Œ†root(X[1])))
    (ll=loglik, H=H)          
end

"""
    loglik(Œ∏, ùí™::ObservationTrajectory) 

    Returns loglikelihood at Œ∏ for one ObservationTrajectory
"""    
function loglik(Œ∏, ùí™::ObservationTrajectory) 
    @unpack X, Y = ùí™
    N = length(Y) 
    h = h_from_observation(Œ∏, Y[N])
    loglik = zero(Œ∏[1][1])
    for i in N:-1:2
        h = pullback(Œ∏, X[i], h) .* h_from_observation(Œ∏, Y[i-1])
        #c = normalise!(h)
        h, c = normalise(h)
        loglik += c
    end
    loglik + log(dot(h, Œ†root(X[1])))
end


"""
    loglik(Œ∏, ùí™s::Vector)

    Returns loglikelihood at Œ∏ for multplies ObservationTrajectories
"""    
function loglik(Œ∏, ùí™s::Vector)
    ll = zero(Œ∏[1][1])
    for i ‚àà eachindex(ùí™s)
        ll += loglik(Œ∏, ùí™s[i])
    end
    ll 
end

loglik(ùí™) = (Œ∏) -> loglik(Œ∏, ùí™) 

‚àáloglik(ùí™) = (Œ∏) -> ForwardDiff.gradient(loglik(ùí™), Œ∏)

# check
function sample_guided(Œ∏, ùí™, H)# Generate approximate track
    X = ùí™.X
    N = length(H) # check -1?
    uprev = sample(Weights(Œ†root(X[1]) .* H[1])) # Weighted prior distribution
    u·µí = [uprev]
    for i=2:N
        w = Ki(Œ∏,X[i])[uprev,:] .* H[i]         # Weighted transition density
        u = sample(Weights(w))
        push!(u·µí, u)
        uprev = u
    end
    u·µí
end

function unitvec(k,K)
    ee = zeros(K); 
    ee[k] = 1.0
    SVector{K}(ee)
end

function viterbi(Œ∏, ùí™::ObservationTrajectory) 
    @unpack X, Y = ùí™
    N = length(Y) 
    h = h_from_observation(Œ∏, Y[N])
    mls = [argmax(h)]  # m(ost) l(ikely) s(tate)
    h = unitvec(mls[1], NUM_HIDDENSTATES)
    #loglik = zero(Œ∏[1][1])
    for i in N:-1:2
        h = pullback(Œ∏, X[i], h) .* h_from_observation(Œ∏, Y[i-1])
        #c = normalise!(h)
        pushfirst!(mls, argmax(h))
        h = unitvec(mls[1], NUM_HIDDENSTATES)
     #   loglik += c
    end
    #loglik + log(dot(h, Œ†root(X[1])))
    mls
end


############ use of Turing to sample from the posterior ################

# model with Œªvector the same for all questions (less parameters)
@model function logtarget(ùí™s)
    Œ≥up ~ filldist(Normal(0,5), DIM_COVARIATES)#MvNormal(fill(0.0, 2), 2.0 * I)
    Œ≥down ~ filldist(Normal(0,5), DIM_COVARIATES)  #MvNormal(fill(0.0, 2), 2.0 * I)
    Z0 ~ filldist(Exponential(), NUM_HIDDENSTATES) 
    Turing.@addlogprob! loglik(ComponentArray(Œ≥12 = Œ≥up, Œ≥21 = Œ≥down, Œ≥23 = Œ≥up, Œ≥32 = Œ≥down, Z1=Z0, Z2=Z0, Z3=Z0, Z4=Z0), ùí™s)
end

@model function logtarget_large(ùí™s)
    Œ≥up ~ filldist(Normal(0,5), DIM_COVARIATES)#MvNormal(fill(0.0, 2), 2.0 * I)
    Œ≥down ~ filldist(Normal(0,5), DIM_COVARIATES)  #MvNormal(fill(0.0, 2), 2.0 * I)
    
    Z1 ~ filldist(Exponential(), NUM_HIDDENSTATES) 
    Z2 ~ filldist(Exponential(), NUM_HIDDENSTATES) 
    Z3 ~ filldist(Exponential(), NUM_HIDDENSTATES) 
    Z4 ~ filldist(Exponential(), NUM_HIDDENSTATES) 
    Turing.@addlogprob! loglik(ComponentArray(Œ≥12 = Œ≥up, Œ≥21 = Œ≥down, Œ≥23 = Œ≥up, Œ≥32 = Œ≥down, Z1=Z1, Z2=Z2, Z3=Z3, Z4=Z4), ùí™s)
end

mapallZtoŒª(Œ∏) = hcat(mapZtoŒª(Œ∏.Z1), mapZtoŒª(Œ∏.Z2), mapZtoŒª(Œ∏.Z3), mapZtoŒª(Œ∏.Z4))

"""
    convert_turingoutput(optimised_model)

    Converts values of Z vectors to Œª vectors

    Example usage: 
    map_estimate = optimize(model, MAP())
    convert_turingoutput(map_estimate)
"""
function convert_turingoutput(optimised_model)
    Œ∏ =  optimised_model.values
    ComponentArray(Œ≥12=[Œ∏[Symbol("Œ≥up[1]")], Œ∏[Symbol("Œ≥up[2]")]],
                      Œ≥21=[Œ∏[Symbol("Œ≥down[1]")], Œ∏[Symbol("Œ≥down[2]")]],
                      Z1=[Œ∏[Symbol("Z1[1]")], Œ∏[Symbol("Z1[2]")], Œ∏[Symbol("Z1[3]")]],
                      Z2=[Œ∏[Symbol("Z2[1]")], Œ∏[Symbol("Z2[2]")], Œ∏[Symbol("Z2[3]")]],
                      Z3=[Œ∏[Symbol("Z3[1]")], Œ∏[Symbol("Z3[2]")], Œ∏[Symbol("Z3[3]")]],
                      Z4=[Œ∏[Symbol("Z4[1]")], Œ∏[Symbol("Z4[2]")], Œ∏[Symbol("Z4[3]")]]
                      )
end
